{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Intermediate HW\n",
    "Change the treshold from 0.5 to 0.2, and rerun the code.\n",
    "\n",
    "Give a commentary in plain English about how that changed precision and recall. What does that mean? What is now included that wasn't before? What part of it is good? What is bad from our Task perspective. Remember: our task was to identify Dissatisfied reviews.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Precision_and_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training example number 6520\n",
      "True rating = 2.000000 stars\n",
      "Expected to be dissatisfied: True\n",
      "Expected probability of being dissatisfied : 0.996439\n",
      "Features = 0.000000 / 1.000000\n",
      "Review text = boring\n",
      "Training example number 853\n",
      "True rating = 5.000000 stars\n",
      "Expected to be dissatisfied: False\n",
      "Expected probability of being dissatisfied : 0.000193\n",
      "Features = 1.000000 / 0.000000\n",
      "Review text = Love it!\n"
     ]
    }
   ],
   "source": [
    "##Change the treshold from 0.5 to 0.2, and rerun the code.\n",
    "pred2A_train = prob2_train > 0.2\n",
    "\n",
    "def analyze_training_example_2A(i):\n",
    "    print(\"Training example number\", i)\n",
    "    print(\"True rating = %f stars\" % Y_train[i])\n",
    "    print(\"Expected to be dissatisfied:\", pred2A_train[i])\n",
    "    print(\"Expected probability of being dissatisfied : %f\" % prob2_train[i])\n",
    "    print(\"Features = %f / %f\" % (X_train[i,0], X_train[i,1]))\n",
    "    print(\"Review text = %s\" % apps_train[i]['reviewText'])\n",
    "    \n",
    "analyze_training_example_2A(max_prob2)\n",
    "analyze_training_example_2A(min_prob2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW text\n",
    "Precision reduced and recall increased! Whenever there is more than 20% probability that the review might be bad, we are flagging it, 20% is a small number honestly to flag a review. On the otherhand, the more we are trying to help people and manually check reviews(even with 80% good probability to be a good review), the more time consuming human work has to be done. So the power of machine learning and automation isn't used to it's potential. It might be good that we are certainly getting most bad reviews and checking them, so that there are no loopholes, but from task perspective the bad part is the reduced automation of flagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the default threshold (0.5) we get precision = 0.650405 and recall = 0.227147\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision2 = precision_score(D_train, pred2_train)\n",
    "recall2 = recall_score(D_train, pred2_train)\n",
    "print(\"For the default threshold (0.5) we get precision = %f \"\n",
    "      \"and recall = %f\" % (precision2, recall2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/330px-Precisionrecall.svg.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We only found 22% of the dissatisfied customers, missed 78% of dissatisfied customers(false negatives).\n",
    "#### Out of the ones we flagged, 65% are actually dissatified, 35% are actually okay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW precision recall check (verify with text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the default threshold (0.2) we get precision = 0.371991 and recall = 0.841968\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision2A = precision_score(D_train, pred2A_train)\n",
    "recall2A = recall_score(D_train, pred2A_train)\n",
    "print(\"For the default threshold (0.2) we get precision = %f \"\n",
    "      \"and recall = %f\" % (precision2A, recall2A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision REDUCED from 65% to 37% \n",
    "## Recall INCREASED from 22% to 84%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
