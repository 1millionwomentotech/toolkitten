{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1: Introduction to Machine Learning for Sentiment Analysis\n",
    "\n",
    "Today we will learn to build a very simple sentiment predictor, which will allow us to forecast the rating of short product reviews on a scale from very positive (5 stars) to very negative (1 star), based on the content of the text only: we will pretend that we can only see the text, and try if we can predict how many stars the user gave the product based on its tone. This is a useful exercise for situations in which we don't have a \"star rating\" easily available. For example, if we are running our own business, many users will probably write tweets or Facebook posts in which they state their opinion of the business, and we want to be able to quickly sort the good from the bad reviews.\n",
    "\n",
    "For this project, we start by setting up our Python environment, and downloading a couple of example datasets (Amazon product reviews) from the Internet. These data were collected by Julian McAuley, UCSD (http://jmcauley.ucsd.edu/data/amazon/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Baby has already been downloaded to ./data/\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "datadir = './data/'\n",
    "\n",
    "import urllib.request, os, gzip\n",
    "\n",
    "def download_data(dataset_name, datadir):\n",
    "    filename = 'reviews_%s_5.json' % dataset_name\n",
    "    filepath = os.path.join(datadir, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        print(\"Dataset %s has already been downloaded to %s\" % (dataset_name, datadir))\n",
    "    else:\n",
    "        url = 'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/%s.gz' % filename\n",
    "        urllib.request.urlretrieve(url, filepath + \".gz\")\n",
    "        with gzip.open(filepath + \".gz\", 'rb') as fin:\n",
    "            with open(filepath, 'wb') as fout:\n",
    "                fout.write(fin.read())\n",
    "        print(\"Downloaded dataset %s and saved it to %s\" % (dataset_name, datadir))\n",
    "\n",
    "dataset = \"Baby\"\n",
    "download_data(dataset, datadir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we use have been stored in the JSON format, which is a standard format for exchanging data over the Internet. The \"JS\" stands for JavaScript, which we'll learn about in week 5 of the summer of code! But Python allows us to read these data using the json library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 160792 data for dataset Baby\n",
      "{'reviewerName': 'Erin White \"Erin\"', 'reviewTime': '05 20, 2005', 'overall': 5.0, 'hash': -6319293282622464590, 'summary': 'Has more comforts than Medela!', 'asin': 'B0000TYHD2', 'reviewerID': 'A2H4QWDVXARPAU', 'reviewText': \"I bought this pump for my new baby because it just as others below have said it looks more comfortable than others and it is! Including Medela. With my other child I encountered breastfeeding problems and had a horrible cheap pump. Now with my new baby she was born with a heart problem (she is fine now after a long road to recovery) and had to stay in the hospital for an extended length of time. Meanwhile I had other children at home and we live 6 hours away from our family and so I had no choice but to divide my time between the hospital and home, which meant I needed a hospital grade pump originally I rented one from the hospital (Medela) and it made my breasts hurt really bad. On the way home from the hospital I stopped in at Babiesrus and bought this pump because it was on our registry (we studied and found it to be more comfortable and with coupons it was much cheaper than Medela,) and started using it and I have been so happy with it that sometimes I still use it so my husband can give her a bottle! I love that all I have to do is pump it and add a nipple and its ready to go and its also easier to store than regular breastmilk bags from medela and its also less expensive also. I love the quiet operation I can pump while were watching tv and we don't have to turn the tv up!\", 'unixReviewTime': 1116547200, 'helpful': [7, 8]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_data(dataset_name, datadir):\n",
    "    filepath = os.path.join(datadir, 'reviews_%s_5.json' % dataset_name)\n",
    "    if not os.path.exists(filepath):\n",
    "        download_data(dataset_name, datadir)\n",
    "    data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:                            # read file line by line\n",
    "            item_hash = hash(line)                # we will use this later for partitioning our data \n",
    "            item = json.loads(line)               # convert JSON string to Python dict\n",
    "            item['hash'] = item_hash              # add hash for identification purposes\n",
    "            data.append(item)\n",
    "    print(\"Loaded %d data for dataset %s\" % (len(data), dataset_name))\n",
    "    return data\n",
    "\n",
    "# load the data...\n",
    "baby = load_data(dataset, datadir)\n",
    "# ... and have a look at an example item (item number 9426):\n",
    "print(baby[9426])\n",
    "#for i, l in enumerate(baby):\n",
    "#    if l['overall'] == 5.0 and 'horrible' in l['reviewText'] and 'bad' in l['reviewText'] and l['helpful'][0]>0:\n",
    "#        print(i, l['reviewText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the dataset contains 160792 data items, with 9 fields called `reviewTime, asin, reviewerID, reviewText, unixReviewTime, summary, helpful, reviewerName, overall`. For example, this particular review was written on May 20, 2005, by the user 'Erin White \"Erin\"'. Erin summarized the product as \"Has more comforts than Medela!\" and gave it 5 stars (out of 5). 7 out of 8 other users rated this review helpful. We also added a hash, which is a single number summarizing the whole data item. For now we can view it as a random ID that is (extremely likely to be) unique for each data item, a bit like a US social security number.\n",
    "\n",
    "For this project, we will ignore all fields except reviewText and overall (i.e. the overall rating in stars out of five). The idea is to find out if we can infer the star rating (how much the user liked the product) by only looking at the text. This way we can learn how to automatically analyze even texts that come without a star rating, such as Facebook posts or tweets.\n",
    "\n",
    "Before we start building a complicated AI solution, it is good practice to first implement a very simple \"baseline predictor\" and measure its performance. This way we get a feeling how hard (or easy) the problem is, For example, we can check for the presence of certain words with strong positive or negative connotations, such as \"good\" or \"fantastic\" versus \"bad\" or \"poor\". Note that this review contains the words \"horrible\", \"hurt\", \"bad\" and \"problem\", and still received 5 stars. This gives us already a feeling for the difficulty of the sentiment analysis problem.\n",
    "\n",
    "Before we develop our first sentiment predictor, we need to partition our data into a training set, a validation set, and a test set. This is something we should do in all our machine learning projects. The idea is that our predictor might overgeneralize from the examples we show it if we are not careful. This is a bit like a child whom you have shown how an iPad works, and then they try to swipe everything that looks like a screen (the TV, the microwave etc.)\n",
    "\n",
    "For instance, the example review listed above might be the only review with the phrase \"6 hours\" in it, and our predictor might learn a rule that the phrase \"6 hours\" is indicative of a high star rating. Such a rule would be unlikely to generalize well to new reviews that we didn't show to the predictor while it was training. So after training our predictor (using only examples from the training set), we need to be able to measure its performance on reviews it hasn't seen yet, which is what the test set is for. We use the validation set because we may want to explore different predictor variants and get an idea how well they perform on unseen examples before committing to one.\n",
    "\n",
    "Therefore the general procedure when developing a predictor using machine learning is as follows:\n",
    "\n",
    "Train several variants of a predictor using examples from the training set only.\n",
    "Use the predictor performance on the validation set to select a single best predictor among all the possible variants. This step also involves debugging our implementation of the predictor, tweaking its parameters etc.\n",
    "Use the test set to get an idea how well our best predictor is expected to perform on unseen data. This step should only be performed once, at the very end of the experiment: if we tweak our predictor based on its performance on the test set, we simply may find a variant that \"gets lucky\" on the test set, and overestimate its accuracy on truly unseen examples.\n",
    "A common rule of thumb is to use 60% of our data for the training, 20% for validation, and 20% for testing. We could simply take the first 60% of our review data for training, but then we have to assume that the dataset has been \"mixed\" well in advance. We achieve a better randomization by using the hash we computed from the JSON string and checking its modulus 10:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we have 96690 training examples, 31912 validation examples, and 32190 test examples\n"
     ]
    }
   ],
   "source": [
    "def partition_train_validation_test(data):\n",
    "    # 60% : modulus is 0, 1, 2, 3, 4, or 5\n",
    "    data_train = [item for item in data if item['hash']%10<=5]  \n",
    "    # 20% : modulus is 6 or 7\n",
    "    data_valid = [item for item in data if item['hash']%10 in [6,7]] \n",
    "    # 20% : modulus is 8 or 9\n",
    "    data_test  = [item for item in data if item['hash']%10 in [8,9]] \n",
    "    return data_train, data_valid, data_test\n",
    "    \n",
    "baby_train, baby_valid, baby_test = partition_train_validation_test(baby)\n",
    "\n",
    "print(\"Now we have\", len(baby_train), \"training examples,\", len(baby_valid),\n",
    "      \"validation examples, and\", len(baby_test), \"test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction algorithms generally work with numbers instead of text. Therefore we need to preprocess our text by extracting from it numeric \"features\" that we can afterwards feed into our predictor. For our simple baseline predictor, we will use only two features: the frequency of positive words in the review (\"good\", \"great\", \"like\") and the frequency of negative words in the review (\"bad\", \"horrible\", \"dislike\"). Let's call these two features **fpos** and **fneg**. Our hypothesis is that reviews with many positive words (high fpos) are likely to receive 4 or 5 stars, while reviews with many negative words (high fneg) will probably receive 1 or 2 stars. Since reviews differ in their length, it makes sense to express the frequency of positive and negative words as a fraction of the total number of words in the review. Therefore fpos and fneg will be numbers between 0 and 1.\n",
    "\n",
    "For example, assume the review text is \"This is a good, great, fantastic, amazing, wonderful, super product!\". We count six positive words and zero negative words (out of ten words in total), so fpos == 0.6 (6/10) and fneg == 0.0 (0/10). \n",
    "\n",
    "On the other hand, the review \"This is a bad, atrocious, terrible, dreadful, awful, abysmal product!\" will have fpos == 0.0 (0/10) and fneg == 0.6 (6/10). Alternatively we could decide to discount all \"stop words\": these are extremely common words such as \"this\", \"is\" or \"a\", which are required by the syntax of the English language but don't really carry semantic meaning. If we strip out these three words, we have fpos == 0.857 (6/7) in the first example, and fneg == 0.857 in the second example.\n",
    "\n",
    "Writing down all positive and negative words in the English language by hand would be a very long and tedious task. Thankfully we don't have to do this ourselves, since it was already done by Hu and Liu (https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html). The Hu-Liu lexicon of positive and negative opinion words is available as part of NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some positive words: subsidize, enchant, profound, lifesaver, undisputable, restructured, enjoy, compatible, restructuring, solid\n",
      "Some negative words: asinine, loneliness, moronic, obsessive, insolence, hellion, deviously, glut, flicker, incongruous\n",
      "Words that appear in both sets: enviously, envious, enviousness\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "positive_words = set(opinion_lexicon.positive())\n",
    "negative_words = set(opinion_lexicon.negative())\n",
    "\n",
    "import random\n",
    "random.seed(1234)\n",
    "print(\"Some positive words:\", \", \".join(random.sample(positive_words, 10)))\n",
    "print(\"Some negative words:\", \", \".join(random.sample(negative_words, 10)))\n",
    "\n",
    "intersection = positive_words & negative_words\n",
    "print(\"Words that appear in both sets: \" + \", \".join(intersection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While sentiment labeling of individual words can be subjective, most people would agree with the classification as positive or negative for these examples. Notice that the Hu-Liu lexicon contains different word forms: for example, it contains both adjectives (\"wholesome\") and adverbs (\"harshly\"), and verbs appear both in the base form (\"blurt\") and in inflected forms (\"picketing\"). It also contains common misspellings (\"flicering\" instead of \"flickering\"). This is why we will take the words in the reviews as they are (apart from minor preprocessing steps such as converting from upper-case to lower-case), and won't apply more sophisticated preprocessing techniques such as stemming.\n",
    "\n",
    "We can now write a function that takes a review text and outputs the number of positive and negative words in the review as a fraction of the total number of words in the review (excluding stop words and punctuation). Note that we have to deal with the special case when the total number of words is zero, which happens if the review text is empty. Otherwise we'll end up dividing by zero, and you'll remember from high school that this is not allowed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8571428571428571, 0.0)\n",
      "(0.0, 0.8571428571428571)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def my_tokenize(text):\n",
    "    # split text into lower-case tokens, removing all-punctuation tokens and stopwords\n",
    "    tokens = []\n",
    "    for sentence in sent_tokenize(text):\n",
    "        tokens.extend(x for x in word_tokenize(sentence.lower()) \n",
    "                      if x not in eng_stopwords and any(i.isalpha() for i in x))\n",
    "    return tokens\n",
    "\n",
    "def pos_neg_fraction(text):\n",
    "    tokens = my_tokenize(text)\n",
    "    count_pos, count_neg = 0, 0\n",
    "    for t in tokens:\n",
    "        if t in positive_words:\n",
    "            count_pos += 1\n",
    "        if t in negative_words:\n",
    "            count_neg += 1\n",
    "    count_all = len(tokens)\n",
    "    if count_all != 0:\n",
    "        return count_pos/count_all, count_neg/count_all\n",
    "    else:\n",
    "        return 0., 0.\n",
    "    \n",
    "pos_example = 'This is a good, great, fantastic, amazing, wonderful, super product!!!'\n",
    "neg_example = 'This is a bad, atrocious, terrible, dreadful, awful, abysmal product!!!'\n",
    "print(pos_neg_fraction(pos_example))\n",
    "print(pos_neg_fraction(neg_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our two example sentences, this seems to do what we want. Obviously, real reviews usually don't have such an extreme concentration of positive or negative words. Let's find the reviews in our real-world datasets with the highest fraction of positive and negative words, respectively. When exploring the data, it is generally a good idea to restrict ourselves to the training data only, to avoid \"peeking ahead\" and designing our algorithm so that it works well for the test examples we see.\n",
    "\n",
    "For the subsequent analysis, we convert our training data set into a matrix `X_train` with two columns and as many rows as there are examples in the data set. The first column contains the fraction of positive words, while the second column contains the fraction of negative words for each example. numpy.array is the standard way to represent matrices in Python. It provides useful helper functions, such as for finding the maximum in each column.\n",
    "\n",
    "Note that the following cell may take a few minutes to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found a fraction of 100.000000 % positive words for example 10461\n",
      "{'reviewerName': 'S. Broderick \"Sondra\"', 'reviewTime': '07 7, 2014', 'overall': 5.0, 'hash': 6869298777839970922, 'summary': 'Five Stars', 'asin': 'B000A88JYQ', 'reviewerID': 'A2FPJGVT01TVVQ', 'reviewText': 'work perfect', 'unixReviewTime': 1404691200, 'helpful': [0, 0]}\n",
      "We found a fraction of 100.000000 % negative words for example 24874\n",
      "{'reviewerName': 'ABDULLAH AL-FALAH', 'reviewTime': '07 4, 2014', 'overall': 2.0, 'hash': 913708519795079022, 'summary': 'Two Stars', 'asin': 'B000WUD83O', 'reviewerID': 'A1SLEYD29KEUW1', 'reviewText': 'too noisy', 'unixReviewTime': 1404432000, 'helpful': [0, 0]}\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "def dataset_to_matrix(data):\n",
    "    return numpy.array([list(pos_neg_fraction(item['reviewText'])) for item in data])\n",
    "\n",
    "X_train = dataset_to_matrix(baby_train)\n",
    "most_pos, most_neg = numpy.argmax(X_train, axis=0)\n",
    "# print the example with the highest fraction of positive words:\n",
    "print(\"We found a fraction of %f %% positive words for example %d\" % \n",
    "      (100.*X_train[most_pos, 0], most_pos))\n",
    "print(baby_train[most_pos])\n",
    "print(\"We found a fraction of %f %% negative words for example %d\" %\n",
    "      (100.*X_train[most_neg, 1], most_neg))\n",
    "print(baby_train[most_neg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there actually is an example with 100% positive words (excluding stopwords), which received 5 stars, and also an example with 100% negative words, which received 1 star. Our idea of counting positive and negative words seems promising! We are now almost ready to train our first predictor. The only thing left to do is to collect the numbers we want to predict (the star ratings which are called `overall` in the JSON data), and put them into another NumPy array. We'll call this one **Y_train**.\n",
    "\n",
    "In machine learning parlance, the matrix **X_train** is called the feature matrix (what we already know) and the matrix **Y_train** is called the target vector (what we want to predict based on the features). You may remember from high-school algebra that vectors are one-dimensional while matrices are two-dimensional. This is because for every example we may have multiple features, but usually a single target. In this case we have two features (fractions of positive and negative examples), so our feature matrix has two columns.\n",
    "\n",
    "Generally, most machine learning algorithms prefer dealing with numbers (matrices) - so we have to find a way to extract numerical information from non-numerical data such as text. This is exactly what we did when counting the fraction of positive and negative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our feature matrix is two-dimensional and has shape (96690, 2)\n",
      "Our target vector is one-dimensional and has shape (96690,)\n"
     ]
    }
   ],
   "source": [
    "def dataset_to_targets(data):\n",
    "    return numpy.array([item['overall'] for item in data])\n",
    "\n",
    "Y_train = dataset_to_targets(baby_train)\n",
    "print(\"Our feature matrix is two-dimensional and has shape\", X_train.shape)\n",
    "print(\"Our target vector is one-dimensional and has shape\", Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a real machine learning project, now would be a good time to spend more time exploring and visualizing the data. For example, it is a good idea to get a feeling how the features and targets are distributed, as this determines which techniques are a good fit for the problem. The Python community has created a lot of great tools and libraries for exploratory data analysis - unfortunately we don't have the time to discuss them in detail right now.\n",
    "\n",
    "However, at least we can give you some pointers if you want to study this topic for yourself. We recommend that anyone interested in becoming a data scientist should have a look at the Pandas library. For example, if you want to visualize how many 1-star, 2-star etc. ratings there are in the dataset, you can do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1376e3f28>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEGCAYAAACD7ClEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJKlhhoAkNcwQkKSG/X+tGEI+aReRsAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1376e3470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas\n",
    "pandas.Series(Y_train).value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that 5-star review are by far the most frequent, and very few people leave a 1-star review. We can also visualize the distribution of our features. We see that most reviews don't have negative words at all (sharp peak around 0), while reviews without any positive words are much rarer. The typical review seems to contain around 10% positive words, and around 5% negative words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1377095c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCUFERAAlBBERiVFCEBERAP4/U1bj/sskQwgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13642acf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pandas.DataFrame(data=X_train, columns = ['fpos', 'fneg'])\n",
    "df.plot.kde(xlim=(-0.05, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two examples are just meant to whet your appetite. To find out more, have a look at the Pandas documentation (https://pandas.pydata.org/pandas-docs/stable/tutorials.html).\n",
    "\n",
    "# Predictor\n",
    "\n",
    "We are now ready to train our first predictor! Generally it's a good idea to keep it simple at the beginning, so we'll start with a method you will probably remember from high school: linear regression (also known as least-squares regression). If you haven't studied the mathematics before or would like to refresh your memory, there are many good materials on the Internet, e.g. on Khanacademy: https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data Otherwise you can also use the Python libraries as a \"black box\" - this will be sufficient if you don't plan on working with data a lot.\n",
    "\n",
    "Later this week we'll learn how to use Google's Tensorflow library to train a linear regression model: however, that will be mostly for pedagogical purposes, and would be a bit of an overkill in practice. The Scikit-learn library (http://scikit-learn.org/stable/) has very good implementations of classical predictors such as linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficient for the fpos variable is 3.214006165353091\n",
      "The coefficient for the fneg variable is -5.7424008967444715\n",
      "The intercept is 4.010060231846752\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lreg = LinearRegression().fit(X_train, Y_train)\n",
    "\n",
    "print(\"The coefficient for the fpos variable is\", lreg.coef_[0])\n",
    "print(\"The coefficient for the fneg variable is\", lreg.coef_[1])\n",
    "print(\"The intercept is\", lreg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like this, we have trained our first machine learning model! Since linear regression is a very simple predictor, we can actually understand its inner workings quite well. The intercept is the star rating we would expect for a review that contains neither positive nor negative words (fpos==0 and fneg==0): according to the model, such a review should get about 4 stars on average.\n",
    "\n",
    "If the review contains 20% positive words (fpos==0.2) but still no negative words (fneg==0), we would expect the following rating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected rating is 4.652861 stars\n",
      "This is the same as 4.652861 stars\n"
     ]
    }
   ],
   "source": [
    "features = [[0.2, 0]]\n",
    "expected_rating_A = lreg.predict(features)[0]\n",
    "print(\"The expected rating is %f stars\" % expected_rating_A)\n",
    "# we can also compute this explicitly:\n",
    "expected_rating_B = lreg.intercept_ + 0.2*lreg.coef_[0] + 0*lreg.coef_[1]\n",
    "print(\"This is the same as %f stars\" % expected_rating_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if the review contains no positive words (fpos==0) but 20% negative words (fneg==0.2), we expect the following rating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected rating is 2.861580 stars\n",
      "This is the same as 2.861580 stars\n"
     ]
    }
   ],
   "source": [
    "features = [[0, 0.2]]\n",
    "expected_rating_A = lreg.predict(features)[0]\n",
    "print(\"The expected rating is %f stars\" % expected_rating_A)\n",
    "# we can also compute this explicitly:\n",
    "expected_rating_B = lreg.intercept_ + 0 * lreg.coef_[0] + 0.2 * lreg.coef_[1]\n",
    "print(\"This is the same as %f stars\" % expected_rating_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the more positive words we have in our review, the higher the expected star rating (the coefficient for the first variable is positive). \n",
    "\n",
    "The more negative words there are in the review, however, the lower is the expected star rating (the coefficient for the second variable is negative). This is what we would intuitively expect.\n",
    "\n",
    "Remember our two examples from earlier, which contained 100% positive words (\"so cute\") or 100% negative words (\"uncomfortable\"). For these two extreme examples, we get a very odd prediction\n",
    "\n",
    "## Intermediate homework for certification:\n",
    "\n",
    "- calculate the prediction for 100% pos, and 100% neg review\n",
    "- repeat this same process for \"Apps for Android\" dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we haven't told our predictor that all ratings must lie between 1 and 5 stars, it became a bit overenthusiastic in its predictions for these extreme examples. We can simply \"cut off\" these unrealistic results: if the predicted star rating is above 5 stars, we set it to 5 stars, and if it is below 1 star, we set it to 1 star. Now we have a practical prediction algorithm, which we can apply to our training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_lreg(features):\n",
    "    expected_rating = lreg.predict(features)\n",
    "    expected_rating[expected_rating > 5.0] = 5.0\n",
    "    expected_rating[expected_rating < 1.0] = 1.0\n",
    "    return expected_rating\n",
    "\n",
    "pred_train = predict_lreg(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some random examples first to get a feeling how well this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training example number 0\n",
      "True rating = 5.000000 stars, expected rating = 4.367172 stars\n",
      "Features = 0.111111 / 0.000000\n",
      "Review text = Perfect for new parents. We were able to keep track of baby's feeding, sleep and diaper change schedule for the first two and a half months of her life. Made life easier when the doctor would ask questions about habits because we had it all right there!\n",
      "Training example number 10000\n",
      "True rating = 5.000000 stars, expected rating = 4.384461 stars\n",
      "Features = 0.210526 / 0.052632\n",
      "Review text = After a few months of use I am very happy with this blanket. It is durable and waterproof and folds out easily with one hand. I was concerned about putting it in the washer but have done so a few times on the delicate cycle and it is holding up perfectly.\n",
      "Training example number 20000\n",
      "True rating = 1.000000 stars, expected rating = 4.469204 stars\n",
      "Features = 0.142857 / 0.000000\n",
      "Review text = Replaced by a bizzilion phone apps.  Would have been awesome in the 90s.\n",
      "Training example number 30000\n",
      "True rating = 5.000000 stars, expected rating = 4.955356 stars\n",
      "Features = 0.294118 / 0.000000\n",
      "Review text = I love this set, and I was so happy to find it on amazon.com for cheaper than I could buy it in the stores.  I think it is simple and looks great.  I also love the color pattern!\n",
      "Training example number 50000\n",
      "True rating = 4.000000 stars, expected rating = 4.039030 stars\n",
      "Features = 0.084507 / 0.042254\n",
      "Review text = When my newborn arrived, the boppy didn't have an adapter so he kept rolling inwards, rendering it useless. After visiting an LC that had a Brest Friend, I purchased the travel version instead of the full version. And, it was perfect. The plastic inside was helpful when he had a spit-up, because I could just strip the cover, wipe down the inside, and voila, clean. When we traveled, I deflated it and packed it in the case. I was very glad not to have the 'full' version and when he is weaned, I can just pack it up without much space taken up.The downside is, the back part is a little unwiedly and I didn't find it super supportive, so I just flapped it off to the side. Also the strap is kind of unnecessary--you don't really need to lock yourself into the pillow--so it just ends up banging around, especially at night.\n"
     ]
    }
   ],
   "source": [
    "def analyze_training_example(i):\n",
    "    print(\"Training example number\", i)\n",
    "    print(\"True rating = %f stars, expected rating = %f stars\" % (Y_train[i], \n",
    "                                                                  pred_train[i]))\n",
    "    print(\"Features = %f / %f\" % (X_train[i,0], X_train[i,1]))\n",
    "    print(\"Review text = %s\" % baby_train[i]['reviewText'])\n",
    "\n",
    "for i in [0, 10000, 20000, 30000, 50000]:\n",
    "    analyze_training_example(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not terrible, but far from perfect either. These few examples already show some of the limitations of our approach. For example, example 50000 is a 1-star review which contains only a single negative word (\"hangs\"), so the predictor overestimates the rating.\n",
    "\n",
    "Example 20000 likewise overestimates the rating because it counts words like \"top\" and \"fabulous\", which don't refer to the product itself in this example.\n",
    "\n",
    "Example 30000 is not too far off, but it underestimates the rating because it counts \"regret\" as a negative word and overlooks the preceding \"not\". Sophisticated sentiment analyzers need to account for negation: for example, \"not bad\" should be treated differently from \"bad\". Intensifiers should also be accounted for: in example 10000, \"very disappointed\" is stronger than \"disappointed\", but our predictor doesn't know this.\n",
    "\n",
    "While looking at individual examples is important and instructive, we need a systematic way to measure the prediction quality across all examples. Scikit-learn provides different evaluation metrics. The conceptually easiest choice is the **mean absolute error**, which counts by how many stars our predictions are off on average (in either direction).\n",
    "\n",
    "For example, assume we have three examples with true star ratings of 1, 4, 5, and predicted star ratings of 5, 4, 3. Then the first example is off by 4 stars, the second example by 0 stars (predicted and true rating match exactly), and the third example is off by 2 stars. The mean absolute error is therefore (4 + 0 + 2) / 3 = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean absolute error on the training data is 0.832466 stars\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae_train = mean_absolute_error(pred_train, Y_train)\n",
    "print(\"The mean absolute error on the training data is %f stars\" % mae_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it turns out that we actually aren't that far off, although the accuracy on the validation set is a bit worse than on the training set (which should be expected). Once we move to more complex algorithms, however, we'll have to be very careful not to draw premature conclusions from the training set performance: an algorithm may easily achieve perfect accuracy on the training set, and still completely fail on unseen examples! (Like a student with a photographic memory who can reproduce all the answers to all the math problems she has seen before, but hasn't understood the underlying general theory: so she cannot compute any answer if we change the numbers in the problem slightly).\n",
    "\n",
    "We don't look at the accuracy on the test set yet, because this is the very last step we should do once we are convinced we have found the best predictor we can think of and want to run one final test. For now, we have established that linear regression with the fraction of positive and negative words as features seems to be a reasonable baseline. Can we do better? Stay tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Certification\n",
    "\n",
    "## Beginner - none\n",
    "\n",
    "## Intermediate \n",
    "- calculate the prediction for 100% pos, and 100% neg review, before the cutoff\n",
    "- repeat this same process for \"Apps for Android\" dataset\n",
    "\n",
    "## Advanced\n",
    "- build a better sentiment analyzer and comment both your code and your data exploration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
