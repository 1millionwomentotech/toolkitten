{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Day 1 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise #5 in http://www.nltk.org/book/ch01.html \n",
    "# Compare the lexical diversity scores for humor and romance fiction in 1.1. Which genre is more lexically diverse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So the genre humor is more lexically diverse.'"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_diversity_humor = 0.231 # the number of distinct words is 23% of the total number of words.\n",
    "lexical_diversity_fiction_romance = 0.121 # the number of distinct words is 12% of the total number of words.\n",
    "\n",
    "'So the genre humor is more lexically diverse.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise #6 in http://www.nltk.org/book/ch01.htmland \n",
    "# Produce a dispersion plot of the four main protagonists in Sense and Sensibility: Elinor, Marianne, Edward, and Willoughby. What can you observe about the different roles played by the males and females in this novel? Can you identify the couples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text2.dispersion_plot([\"Elinor\", \"Marianne\", \"Edward\", \"Willoughby\"]))\n",
    "\n",
    "'It can be seen that the female characters Elinor and Marianne appear much more frequently than the male characters. The dispersion of their apparation is quite spread out over the book.'\n",
    "'Looking at the distribution, it looks like the couples are 1- Elinor and Edward, 2- Marianne and Willoughby.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Day 2 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%pprint\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from nltk.book import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import webtext\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import nps_chat\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise: Type up the whole Chapter 2 of http://www.nltk.org/book/ch02.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Gutenberg Corpus\n",
    "\n",
    "print(gutenberg.fileids())\n",
    "emma = gutenberg.words('austen-emma.txt')\n",
    "len(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fileid in gutenberg.fileids():\n",
    "    num_chars = len(gutenberg.raw(fileid))\n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))\n",
    "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
    "    print (round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')\n",
    "macbeth_sentences\n",
    "macbeth_sentences[1116]\n",
    "longest_len = max(len(s) for s in macbeth_sentences)\n",
    "[s for s in macbeth_sentences if len(s) == longest_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Web and Chat Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fileid in webtext.fileids():\n",
    "    print(fileid, webtext.raw(fileid)[:65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatroom = nps_chat.posts('10-19-20s_706posts.xml')\n",
    "chatroom[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.categories()\n",
    "brown.words(categories = 'news')\n",
    "brown.words(fileids = ['cr06'])\n",
    "brown.sents(categories = ['news', 'humor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_text = brown.words(categories='news')\n",
    "fdist = nltk.FreqDist(w.lower() for w in news_text)\n",
    "modals = ['can','could','may','must']\n",
    "for m in modals:\n",
    "    print (m + \" : \", fdist[m], end=' ')\n",
    "# print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist((genre, word)\n",
    "                              for genre in brown.categories()\n",
    "                              for word in brown.words(categories=genre))\n",
    "genres = ['humor', 'news', 'hobbies']\n",
    "pronouns = ['she', 'her', 'hers', 'he', 'him', 'his', 'it', 'its', 'they', 'them', 'theirs']\n",
    "\n",
    "cfd.tabulate(conditions=genres, samples=pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Reuters Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.fileids()\n",
    "reuters.categories()\n",
    "reuters.categories('training/9865')\n",
    "reuters.categories(['training/9865', 'training/9880'])\n",
    "reuters.fileids('barley')\n",
    "reuters.fileids(['barley', 'corn'])\n",
    "reuters.words('training/9865')[:14]\n",
    "reuters.words(['training/9865', 'training/9880'])\n",
    "reuters.words(categories='barley')\n",
    "reuters.words(categories=['barley', 'corn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 Inaugural Adress Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural.fileids()\n",
    "[fileid[:4] for fileid in inaugural.fileids()]\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "            (target, fileid[:4])\n",
    "            for fileid in inaugural.fileids()\n",
    "            for w in inaugural.words(fileid)\n",
    "            for target in ['america', 'citizen']\n",
    "            if w.lower().startswith(target))\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.7 Corpora in Other Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.cess_esp.words()\n",
    "nltk.corpus.floresta.words()\n",
    "nltk.corpus.indian.words('hindi.pos')\n",
    "nltk.corpus.udhr.fileids()\n",
    "nltk.corpus.udhr.words('Javanese-Latin1')[11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr\n",
    "languages = ['Chickasaw', 'English', 'German_Deutsch','Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "            (lang, len(word))\n",
    "            for lang in languages\n",
    "            for word in udhr.words(lang + '-Latin1'))\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.8 Text Corpus Structure\n",
    "\n",
    "raw = gutenberg.raw(\"burgess-busterbrown.txt\")\n",
    "raw[1:20]\n",
    "words = gutenberg.words(\"burgess-busterbrown.txt\")\n",
    "words[1:20]\n",
    "sents = gutenberg.sents(\"burgess-busterbrown.txt\")\n",
    "sents[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Conditional Frequency Distributions\n",
    "# 2.2   Counting Words by Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "            (genre, word)\n",
    "            for genre in brown.categories()\n",
    "            for word in brown.words(categories=genre)\n",
    "            )\n",
    "genre_word = [(genre,word) for genre in ['news', 'romance'] for word in brown.words(categories=genre)]\n",
    "len(genre_word)\n",
    "genre_word[:4]\n",
    "genre_word[-4:]\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(genre_word)\n",
    "cfd\n",
    "cfd.conditions()\n",
    "\n",
    "print(cfd['news'])\n",
    "cfd['news']['expected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3   Plotting and Tabulating Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "            (target, fileid[:4])\n",
    "            for fileid in inaugural.fileids()\n",
    "            for w in inaugural.words(fileid)\n",
    "            for target in ['america', 'citizen'] \n",
    "            if w.lower().startswith(target)\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr\n",
    "languages = ['Chickasaw', 'English', 'German_Deutsch', 'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "            (lang, len(word)) \n",
    "            for lang in languages\n",
    "            for word in udhr.words(lang + '-Latin1'))\n",
    "\n",
    "cfd.tabulate(conditions=['English', 'German_Deutsch'],samples=range(10), cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4   Generating Random Text with Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven','and', 'the', 'earth', '.']\n",
    "list(nltk.bigrams(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(cfdist, word, num=15):\n",
    "    for i in range(num):\n",
    "        print(word, end=' ')\n",
    "        word = cfdist[word].max()\n",
    "\n",
    "text = nltk.corpus.genesis.words('english-kjv.txt')\n",
    "bigrams = nltk.bigrams(text)\n",
    "cfd = nltk.ConditionalFreqDist(bigrams) \n",
    "cfd['living']\n",
    "generate_model(cfd, 'living')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3   More Python: Reusing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "def lexical_diversity(my_text_data):\n",
    "    word_count = len(my_text_data)\n",
    "    vocab_size = len(set(my_text_data))\n",
    "    diversity_score = vocab_size / word_count\n",
    "    return diversity_score\n",
    "\n",
    "from nltk.corpus import genesis\n",
    "kjv = genesis.words('english-kjv.txt')\n",
    "lexical_diversity(kjv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plural(word):\n",
    "    if word.endswith('y'):\n",
    "        return word[:-1] + 'ies'\n",
    "    elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:\n",
    "        return word + 'es'\n",
    "    elif word.endswith('an'):\n",
    "        return word[:-2] + 'en'\n",
    "    else:\n",
    "        return word + 's'\n",
    "\n",
    "plural('fairy')\n",
    "plural('woman')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_proc import plural\n",
    "# frunctions saved another file called text_proc.py\n",
    "plural('wish')\n",
    "plural('fan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4   Lexical Resources\n",
    "# 4.1   Wordlist Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unusual_words(text):\n",
    "    text_vocab = set(w.lower() for w in text if w.isalpha())\n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    unusual = text_vocab - english_vocab\n",
    "    return sorted(unusual)\n",
    "\n",
    "unusual_words(nltk.corpus.gutenberg.words('austen-sense.txt'))\n",
    "unusual_words(nltk.corpus.nps_chat.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n",
    "\n",
    "def content_fraction(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return len(content) / len(text)\n",
    "\n",
    "content_fraction(nltk.corpus.reuters.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puzzle_letters = nltk.FreqDist('egivrvonl')\n",
    "obligatory = 'r'\n",
    "puzzle_letters\n",
    "wordlist = nltk.corpus.words.words()\n",
    "[w for w in wordlist if len(w) >= 6 \n",
    "         and obligatory in w \n",
    "         and nltk.FreqDist(w) <= puzzle_letters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = nltk.corpus.names\n",
    "names.fileids()\n",
    "male_names = names.words('male.txt')\n",
    "female_names = names.words('female.txt')\n",
    "[w for w in male_names if w in female_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (fileid, name[-1])\n",
    "    for fileid in names.fileids()\n",
    "    for name in names.words(fileid))\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.2   A Pronouncing Dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = nltk.corpus.cmudict.entries()\n",
    "len(entries)\n",
    "for entry in entries[42371:42379]:\n",
    "    print(entry)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, pron in entries:\n",
    "    if len(pron) == 3:\n",
    "        ph1, ph2, ph3 = pron\n",
    "        if ph1 == 'P' and ph3 == 'T':\n",
    "            print(word, ph2, end=' ')\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllable = ['N', 'IH0', 'K', 'S']\n",
    "[word for word, pron in entries if pron[-4:] == syllable]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w, pron in entries if pron[-1] == 'M' and w[-1] == 'n']\n",
    "sorted(set(w[:2] for w, pron in entries if pron[0] == 'N' and w[0] != 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stress(pron):\n",
    "    return [char for phone in pron for char in phone if char.isdigit()]\n",
    "[w for w, pron in entries if stress(pron) == ['0', '1', '0', '2', '0']]\n",
    "[w for w, pron in entries if stress(pron) == ['0', '2', '0', '1', '0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = [(pron[0]+'-'+pron[2], word)\n",
    "      for (word, pron) in entries\n",
    "      if pron[0] == 'P' and len(pron) == 3] \n",
    "cfd = nltk.ConditionalFreqDist(p3)\n",
    "for template in sorted(cfd.conditions()):\n",
    "    if len(cfd[template]) > 10:\n",
    "        words = sorted(cfd[template])\n",
    "        wordstring = ' '.join(words)\n",
    "        print(template, wordstring[:70] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prondict = nltk.corpus.cmudict.dict()\n",
    "prondict['fire']\n",
    "# prondict['blog'] \n",
    "prondict['blog'] = [['B', 'L', 'AA1', 'G']] \n",
    "prondict['blog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['natural', 'language', 'processing']\n",
    "[ph for w in text for ph in prondict[w][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3   Comparative Wordlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import swadesh\n",
    "swadesh.fileids()\n",
    "swadesh.words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr2en = swadesh.entries(['fr', 'en'])\n",
    "fr2en\n",
    "translate = dict(fr2en)\n",
    "translate['chien']\n",
    "translate['jeter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de2en = swadesh.entries(['de', 'en'])    \n",
    "es2en = swadesh.entries(['es', 'en']) \n",
    "translate.update(dict(de2en))\n",
    "translate.update(dict(es2en))\n",
    "translate['Hund']\n",
    "translate['perro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['en', 'de', 'nl', 'es', 'fr', 'pt', 'la']\n",
    "for i in [139, 140, 141, 142]:\n",
    "    print(swadesh.entries(languages)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4   Shoebox and Toolbox Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import toolbox\n",
    "toolbox.entries('rotokas.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5   WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "wn.synsets('motorcar')\n",
    "\n",
    "wn.synset('car.n.01').lemma_names()\n",
    "\n",
    "wn.synset('car.n.01').definition()\n",
    "wn.synset('car.n.01').examples()\n",
    "\n",
    "wn.synset('car.n.01').lemmas() \n",
    "wn.lemma('car.n.01.automobile') \n",
    "wn.lemma('car.n.01.automobile').synset() \n",
    "wn.lemma('car.n.01.automobile').name()\n",
    "\n",
    "wn.synsets('car')\n",
    "for synset in wn.synsets('car'):\n",
    "    print(synset.lemma_names())\n",
    "\n",
    "wn.lemmas('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2   The WordNet Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motorcar = wn.synset('car.n.01')\n",
    "types_of_motorcar = motorcar.hyponyms()\n",
    "types_of_motorcar[0]\n",
    "sorted(lemma.name() for synset in types_of_motorcar for lemma in synset.lemmas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motorcar.hypernyms()\n",
    "paths = motorcar.hypernym_paths()\n",
    "len(paths)\n",
    "[synset.name() for synset in paths[0]]\n",
    "[synset.name() for synset in paths[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motorcar.root_hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3   More Lexical Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('tree.n.01').part_meronyms()\n",
    "wn.synset('tree.n.01').substance_meronyms()\n",
    "wn.synset('tree.n.01').member_holonyms()\n",
    "\n",
    "for synset in wn.synsets('mint', wn.NOUN):\n",
    "    print(synset.name() + ':', synset.definition())\n",
    "wn.synset('mint.n.04').part_holonyms()\n",
    "wn.synset('mint.n.04').substance_holonyms()\n",
    "\n",
    "wn.synset('walk.v.01').entailments()\n",
    "wn.synset('eat.v.01').entailments()\n",
    "wn.synset('tease.v.03').entailments()\n",
    "\n",
    "wn.lemma('supply.n.02.supply').antonyms()\n",
    "wn.lemma('rush.v.01.rush').antonyms()\n",
    "wn.lemma('horizontal.a.01.horizontal').antonyms()\n",
    "wn.lemma('staccato.r.01.staccato').antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4   Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = wn.synset('right_whale.n.01')\n",
    "orca = wn.synset('orca.n.01')\n",
    "minke = wn.synset('minke_whale.n.01')\n",
    "tortoise = wn.synset('tortoise.n.01')\n",
    "novel = wn.synset('novel.n.01')\n",
    "right.lowest_common_hypernyms(minke)\n",
    "right.lowest_common_hypernyms(orca)\n",
    "right.lowest_common_hypernyms(tortoise)\n",
    "right.lowest_common_hypernyms(novel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('baleen_whale.n.01').min_depth()\n",
    "wn.synset('whale.n.02').min_depth()\n",
    "wn.synset('vertebrate.n.01').min_depth()\n",
    "wn.synset('entity.n.01').min_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right.path_similarity(minke)\n",
    "right.path_similarity(orca)\n",
    "right.path_similarity(tortoise)\n",
    "right.path_similarity(novel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise 8 of http://www.nltk.org/book/ch02.html: Define a conditional frequency distribution over the Names corpus that allows you to see which initial letters are more frequent for males vs. females (cf. 4.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = nltk.corpus.names\n",
    "male_names = names.words('male.txt')\n",
    "female_names = names.words('female.txt')\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "            (fileid, name[0])\n",
    "            for fileid in names.fileids()\n",
    "            for name in names.words(fileid)\n",
    "    )\n",
    "cfd.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise 10 of http://www.nltk.org/book/ch02.html: How many word types account for a third of all word tokens, for a variety of text sources? What do you conclude about this statistic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "def third_of_tokens(text):\n",
    "    words_in_text = [w for w in text if any(c.isalpha() for c in w)]\n",
    "\n",
    "    fd = nltk.FreqDist(words_in_text)\n",
    "    most = fd.most_common(1000)\n",
    "    \n",
    "    count = 0\n",
    "    third_words = []\n",
    "\n",
    "    for word, num in most:\n",
    "        if ((count < (len(words_in_text) / 3)) and any(c.isalpha() for c in word)):\n",
    "            count = count + num\n",
    "            third_words.append(word)\n",
    "    print (third_words)        \n",
    "    print (len(third_words))\n",
    "\n",
    "def third_of_tokens_corpus(corpus):\n",
    "    for fileid in corpus.fileids():\n",
    "        print (fileid, third_of_tokens(corpus.words(fileid)))\n",
    "\n",
    "third_of_tokens_corpus(gutenberg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'to', 'the', 'not', 'when', 'on', 'a', 'is', 't', 'and', 'of', 'page', 'for', 'with', 'window', 'Firefox', 'does', 'from', 'open', 'menu', 'should', 'bar', 'tab', 'doesn', 'after', 'Firebird', 'new', 'work', 'browser', 'if', 'be', 'toolbar', 'button', 'bookmarks', 'dialog', 'bookmark', 'as', 'file', 'URL', 'download', 'crashes', 'I', 'no', 'text', 'are', 'or', 'up']\n",
      "47\n",
      "firefox.txt None\n",
      "['the', 'I', 'ARTHUR', 'you', 'a', 'of', 'to', 's', 'and', 'Oh', 'it', 'is', 'in', 'that', 't', 'No', 'LAUNCELOT', 'your', 'not', 'GALAHAD', 'KNIGHT', 'What', 'FATHER', 'we', 'You', 'BEDEVERE', 'We', 'this', 'no', 'Well', 'HEAD', 'have', 'GUARD', 'are', 'Sir', 'A', 'And', 'on', 'VILLAGER', 'Ni', 'me']\n",
      "41\n",
      "grail.txt None\n",
      "['I', 'you', 'the', 'to', 'a', 's', 't', 'and', 'that', 'it', 'Girl', 'on', 'Guy', 'in', 'is', 'like', 'of', 'girl', 'You', 'my', 'me', 'guy', 'm', 'have', 'was', 'know', 'don', 'just', 'for']\n",
      "29\n",
      "overheard.txt None\n",
      "['the', 'a', 'to', 'of', 'Jack', 's', 'I', 'you', 'and', 'is', 'JACK', 'SPARROW', 'in', 'on', 'it', 'his', 'WILL', 'TURNER', 'Will', 'with', 'ELIZABETH', 'SWANN', 'at', 'from', 'GIBBS', 'that', 'Elizabeth', 't', 'up', 'as']\n",
      "30\n",
      "pirates.txt None\n",
      "['for', 'and', 'to', 'lady', 'seeks', 'a', 'with', 'S', 'ship', 'relationship', 'fun', 'in', 'slim', 'build', 'o', 's', 'y', 'smoker', 'non', 'I', 'movies', 'good', 'honest', 'dining', 'out', 'rship', 'looking', 'like', 'age', 'attractive', 'who', 'friendship', 'Looking', 'MALE', 'times', 'male', 'meet', 'life', 'seeking', 'r']\n",
      "40\n",
      "singles.txt None\n",
      "['a', 'and', 'the', 'of', 'but', 'I', 'it', 'to', 'fruit', 'with', 'good', 'A', 'wine', 'this', 'bit', 'in', 'Very', 'quite', 'Top', 'is', 'very', 'that', 'nose', 'touch', 'for', 'on', 'Bare', 'more']\n",
      "28\n",
      "wine.txt None\n"
     ]
    }
   ],
   "source": [
    "third_of_tokens_corpus(webtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PART', 'JOIN', 'i', 'ACTION', 'to', 'you', 'the', 'a', 'lol', 'me', 'I', 'u', 'U115', 'in', 'U121', 'that', 'my', 'U6', 'is', 'it', 'U7', \"'s\", 'do', 'and', 'any', 'he', 'hey', 'your', 'what']\n",
      "29\n",
      "10-19-20s_706posts.xml None\n",
      "['lol', 'I', 'the', 'you', 'to', 'i', 'PART', 'U17', 'JOIN', 'a', 'me', 'it', 'is', 'hey', 'U9', 'in', 'hi', 'U31', 'my', 'no', 'and', 'what', 'for', 'that', 'U25', 'do', 'U11', 'lmao']\n",
      "28\n",
      "10-19-30s_705posts.xml None\n",
      "['hi', 'lol', 'a', 'you', 'JOIN', 'to', 'i', 'I', 'the', 'PART', 'it', 'U11', 'U34', 'is', 'U27', 'in', 'me', 'U52', 'and', 'U9', 'hey', 'U20', 'U30', 'on', 'hiya', 'Hi', 'that', 'U53', 'for', 'do']\n",
      "30\n",
      "10-19-40s_686posts.xml None\n",
      "['i', 'you', 'to', 'the', 'I', 'lol', 'a', 'U35', 'ACTION', 'me', 'JOIN', 'PART', 'U32', 'it', 'U28', 'in', 'is', 'are', 'of', 'U23', 'and', 'that', 'out', 'too', 'U47', 'your', 'not', 'on', 'like', 'U16', 'was', 'my', \"'s\", 'all', \"'m\", 'LoL']\n",
      "36\n",
      "10-19-adults_706posts.xml None\n",
      "['hi', 'lol', 'the', 'to', 'a', 'you', 'is', 'PART', 'JOIN', 'I', 'in', 'me', 'U16', 'U50', 'U41', 'and', 'U37', 'U19', 'hiya', 'lmao', 'for', 'i', 'ACTION', 'that', 'U33', 'U34', 'U12', 'wb', 'U13', 'howdy']\n",
      "30\n",
      "10-24-40s_706posts.xml None\n",
      "['JOIN', 'PART', 'I', 'you', 'i', 'U54', 'a', 'to', 'the', 'ACTION', \"'s\", 'U122', 'it', 'lol', 'me', \"'m\", '#14-19teens', 'is', 'what', 'o', 'this', 'MODE', 'that', 'my']\n",
      "24\n",
      "10-26-teens_706posts.xml None\n",
      "['PART', 'JOIN', 'the', 'I', 'to', 'ACTION', 'lol', 'in', 'you', 'U105', 'is', 'a', 'i', 'me', 'hi', 'hey', 'U99', 'U25', 'and', 'my', 'all', 'here', 'U53', 'can', 'are', \"'s\"]\n",
      "26\n",
      "11-06-adults_706posts.xml None\n",
      "['lol', 'a', 'i', 'JOIN', 'you', 'PART', 'to', 'and', 'is', 'me', 'ACTION', 'U21', 'I', 'the', 'U110', 'that', 'on', 'U65', 'U69', 'my', 'im', 'am', 'U91', 'it', 'hey', 'good', 'virgin', 'for', 'your', 'yes']\n",
      "30\n",
      "11-08-20s_705posts.xml None\n",
      "['hi', 'JOIN', 'lol', 'PART', 'U7', 'the', 'i', 'I', 'U18', 'to', 'hey', 'you', 'a', 'U48', 'U24', 'U66', 'in', 'and', 'what', 'me', 'wb', 'who', 'that', 'know']\n",
      "24\n",
      "11-08-40s_706posts.xml None\n",
      "['JOIN', 'PART', 'i', 'to', 'you', 'the', 'it', 'is', 'a', 'and', 'hey', 'that', 'hi', 'I', 'in', 'on', 'ACTION', 'me', 'lol', 'my', 'do', 'u', 'U3', 'U56', 'room', 'for', 'im', 'all', 'how', 'U65']\n",
      "30\n",
      "11-08-adults_705posts.xml None\n",
      "['PART', 'JOIN', 'me', 'to', 'i', 'chat', 'any', 'a', 'wanna', 'and', 'the', 'u', 'here', 'pm', 'in', 'what', 'guys', 'm', 'hey', 'all', 'haha', 'lol', 'is']\n",
      "23\n",
      "11-08-teens_706posts.xml None\n",
      "['JOIN', 'you', 'PART', 'to', 'i', 'the', 'a', 'I', 'me', 'in', 'it', 'U156', 'ACTION', 'and', 'my', 'of', 'lol', 'is', 'are', 'up', 'U34', 'U89', 'with', 'hey', 'U114', \"'s\", 'on', 'not', 'like', 'u', 'your', 'so', 'hi']\n",
      "33\n",
      "11-09-20s_706posts.xml None\n",
      "['the', 'lol', 'hi', 'a', 'U18', 'you', 'to', 'Hi', 'I', 'and', 'U7', 'PART', 'U52', 'i', 'in', 'JOIN', 'it', 'U34', 'that', 'is', 'U30', 'have', 'of', 'here', 'on', 'U19', 'U49', 'ACTION', 'how', 'are']\n",
      "30\n",
      "11-09-40s_706posts.xml None\n",
      "['PART', 'JOIN', 'i', 'lol', 'to', 'you', 'the', 'a', 'I', 'it', 'is', 'that', 'in', 'and', 'hi', 'me', 'my', 'so', 'no', 'have', 'of', 'ACTION', \"'s\"]\n",
      "23\n",
      "11-09-adults_706posts.xml None\n",
      "['JOIN', 'PART', 'to', 'me', 'I', 'chat', 'the', 'pm', 'you', 'any', 'm', 'a', 'u', 'hey', 'ACTION', 'is', 'wanna', 'in', 'if']\n",
      "19\n",
      "11-09-teens_706posts.xml None\n"
     ]
    }
   ],
   "source": [
    "third_of_tokens_corpus(nps_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Statistics roughly accurate!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminders to myself:\n",
    "# distinct words = \"word types\"\n",
    "# A word \"token\" is a particular appearance of a given word in a text; a word \"type\" is the unique form of the word as a particular sequence of letters. We count word tokens using len(text) and word types using len(set(text)).\n",
    "# We obtain the vocabulary of a text t using sorted(set(t)).\n",
    "# We operate on each item of a text using [f(x) for x in text].\n",
    "# To derive the vocabulary, collapsing case distinctions and ignoring punctuation, we can write set(w.lower() for w in text if w.isalpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Day 3 ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise: Do research to see what Python libraries are already in existence that you could start using in your day-job, or daily life.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is the most popular, and portable game library for python, with over 1000 free and open source projects that use pygame to look at.'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"I'm currently working on Virtual Reality projects and it looks like PyGame would be very useful: \"\n",
    "\"It is a set of Python modules designed for writing games, written on top of SDL library\" \n",
    "\"It allows to create fully featured games and multimedia programs in the python language.\"\n",
    "\"It is the most popular, and portable game library for python, with over 1000 free and open source projects that use pygame to look at.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise 5 ☼ Investigate the holonym-meronym relations for some nouns. Remember that there are three kinds of holonym-meronym relation, so you need to use: member_meronyms(), part_meronyms(), substance_meronyms(), member_holonyms(), part_holonyms(), and substance_holonyms()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Member Meronyms:  [] \n",
      "Part Meronyms:  [] \n",
      "Substance Meronyms:  [Synset('lignin.n.01')] \n",
      "Member Holonyms:  [] \n",
      "Part Holonyms:  [] \n",
      "Substance Holonyms:  [Synset('beam.n.02'), Synset('chopping_block.n.01'), Synset('lumber.n.01'), Synset('spindle.n.02')]\n"
     ]
    }
   ],
   "source": [
    "wn.synsets('airplane')\n",
    "wn.synset('airplane.n.01').member_meronyms()\n",
    "wn.synset('airplane.n.01').part_meronyms()\n",
    "wn.synset('airplane.n.01').substance_meronyms()\n",
    "wn.synset('airplane.n.01').member_holonyms() \n",
    "wn.synset('airplane.n.01').part_holonyms()\n",
    "wn.synset('airplane.n.01').substance_holonyms()\n",
    "\n",
    "wn.synsets('wood')\n",
    "wn.synset('wood.n.01').member_meronyms()\n",
    "wn.synset('wood.n.01').part_meronyms()\n",
    "wn.synset('wood.n.01').substance_meronyms()\n",
    "wn.synset('wood.n.01').member_holonyms() \n",
    "wn.synset('wood.n.01').part_holonyms()\n",
    "wn.synset('wood.n.01').substance_holonyms()\n",
    "\n",
    "def relations(noun):\n",
    "    noun_synset = wn.synset(noun)\n",
    "    print ('Member Meronyms: ', noun_synset.member_meronyms() , \n",
    "           '\\nPart Meronyms: ', noun_synset.part_meronyms() ,\n",
    "           '\\nSubstance Meronyms: ' , noun_synset.substance_meronyms() , \n",
    "           '\\nMember Holonyms: ' , noun_synset.member_holonyms() ,\n",
    "           '\\nPart Holonyms: ' , noun_synset.part_holonyms() , \n",
    "           '\\nSubstance Holonyms: ' , noun_synset.substance_holonyms())\n",
    "\n",
    "relations('wood.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise 13 ◑ What percentage of noun synsets have no hyponyms? You can get all noun synsets using wn.all_synsets('n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('congener.n.03'), Synset('benthos.n.02'), Synset('dwarf.n.03'), Synset('heterotroph.n.01'), Synset('parent.n.02'), Synset('biont.n.01'), Synset('native.n.03'), Synset('absolute_space.n.01'), Synset('thing.n.02'), Synset('abdominoplasty.n.01'), Synset('abort.n.01'), Synset('alienation.n.04'), Synset('application.n.07'), Synset('beachhead.n.02'), Synset('cakewalk.n.02'), Synset('masterpiece.n.02'), Synset('masterstroke.n.01'), Synset('credit.n.04'), Synset('res_gestae.n.02'), Synset('blind_alley.n.02')]\n",
      "65422\n",
      "The percentage of noun synsets with no hyponyms is 79.67119283931072 %\n"
     ]
    }
   ],
   "source": [
    "nouns = list(wn.all_synsets('n'))\n",
    "# print(nouns[:20])\n",
    "no_hyp_noun = []\n",
    "for noun in nouns:\n",
    "    if noun.hyponyms() == []:\n",
    "        no_hyp_noun.append(noun)\n",
    "print(no_hyp_noun[:20])\n",
    "print(len(no_hyp_noun))\n",
    "\n",
    "percentage = (len(no_hyp_noun)/len(nouns))*100\n",
    "print('The percentage of noun synsets with no hyponyms is ' + str(percentage) + ' %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('beaker.n.02'), Synset('chalice.n.01'), Synset('coffee_cup.n.01'), Synset('dixie_cup.n.01'), Synset('grace_cup.n.01'), Synset('kylix.n.01'), Synset('mustache_cup.n.01'), Synset('scyphus.n.01'), Synset('teacup.n.02')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('cup.n.01').hyponyms()\n",
    "# wn.synset('cakewalk.n.02').hyponyms()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise 14 ◑ Define a function supergloss(s) that takes a synset s as its argument and returns a string consisting of the concatenation of the definition of s, and the definitions of all the hypernyms and hyponyms of s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('synset: ', ('cup.n.01', 'a small open container usually used for drinking; usually has a handle'), ' hypernyms: ', [('container.n.01', 'any object that can be used to hold things (especially a large metal boxlike object of standardized dimensions that can be loaded from one form of transport to another)'), ('crockery.n.01', 'tableware (eating and serving dishes) collectively')], ' hoponyms: ', [('beaker.n.02', 'a cup (usually without a handle)'), ('chalice.n.01', 'a bowl-shaped drinking vessel; especially the Eucharistic cup'), ('coffee_cup.n.01', 'a cup from which coffee is drunk'), ('dixie_cup.n.01', 'a disposable cup made of paper; for holding drinks'), ('grace_cup.n.01', 'cup to be passed around for the final toast after a meal'), ('kylix.n.01', 'a shallow drinking cup with two handles; used in ancient Greece'), ('mustache_cup.n.01', \"a drinking cup with a bar inside the rim to keep a man's mustache out of the drink\"), ('scyphus.n.01', 'an ancient Greek drinking cup; two handles and footed base'), ('teacup.n.02', 'a cup from which tea is drunk')])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def supergloss(s):\n",
    "    synset = (s.name(),s.definition())\n",
    "    hypernyms_list = list(s.hypernyms())\n",
    "    hyponyms_list = list(s.hyponyms())\n",
    "    \n",
    "    hypernyms = [(word.name(), word.definition()) for word in hypernyms_list]\n",
    "    hyponyms = [(word.name(), word.definition()) for word in hyponyms_list]\n",
    "\n",
    "    definitions = \"synset: \" , synset , \" hypernyms: \"  , hypernyms , \" hoponyms: \" , hyponyms\n",
    "                  \n",
    "    return (definitions)\n",
    "\n",
    "supergloss(wn.synset('cup.n.01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Day 4 ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise 6-10 from http://www.nltk.org/book/ch03.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise 6: Describe the class of strings matched by the following regular expressions.\n",
    "# [a-zA-Z]+\n",
    "# [A-Z][a-z]*\n",
    "# p[aeiou]{,2}t\n",
    "# \\d+(\\.\\d+)?\n",
    "# ([^aeiou][aeiou][^aeiou])*\n",
    "# \\w+|[^\\w\\s]+\n",
    "# Test your answers using nltk.re_show()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Whole} {World}\n"
     ]
    }
   ],
   "source": [
    "'[a-zA-Z]+ : strings containing one or more letters (capital or not)'\n",
    "nltk.re_show(r'[a-zA-Z]+', 'Whole World')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Whole} world\n"
     ]
    }
   ],
   "source": [
    "'[A-Z][a-z]* : one capital letter and zero or more lowercase letters' \n",
    "nltk.re_show(r'[A-Z][a-z]*', 'Whole world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a {pet} which likes to party\n"
     ]
    }
   ],
   "source": [
    "'p[aeiou]{,2}t : starts with p, followed by 0 up to 2 vowels (aeiou), end with t '\n",
    "nltk.re_show(r'p[aeiou]{,2}t', 'This is a pet which likes to party')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the result is {0.24} or maybe {65}\n"
     ]
    }
   ],
   "source": [
    "'\\d+(\\.\\d+)? : an integer or decimal number'\n",
    "nltk.re_show(r'\\d+(\\.\\d+)?', 'the result is 0.24 or maybe 65')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}M{}y{} {mag}i{}c{} {key}b{}o{}a{}r{}d{ is} {}w{rit}i{}n{}g{} {}t{hin}g{}s{} {}b{}y{ itsel}f{}\n"
     ]
    }
   ],
   "source": [
    "'([^aeiou][aeiou][^aeiou])* : '\n",
    "'zero or more of the following sequence: non vowel, vowel, non-vowel'\n",
    "nltk.re_show(r'([^aeiou][aeiou][^aeiou])*', 'My magic keyboard is writing things by itself')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{My} {magic} {keyboard} {is} {writing} {things} {by} {itself}\n"
     ]
    }
   ],
   "source": [
    "'\\w+|[^\\w\\s]+ : '\n",
    "'zero or one of any alphanumeric characters or zero or one of any characters that are neither alphanumeric nor whitespace'\n",
    "nltk.re_show(r'\\w+|[^\\w\\s]+', 'My magic keyboard is writing things by itself')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise 7: Write regular expressions to match the following classes of strings:\n",
    "# a. A single determiner (assume that a, an, and the are the only determiners).\n",
    "# b. An arithmetic expression using integers, addition, and multiplication, such as 2*3+8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.\n",
    "'^(a|an|the)$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b.\n",
    "'\\d+([\\+\\*]\\d+)+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise 8: Write a utility function that takes a URL as its argument, \n",
    "# and returns the contents of the URL, with all HTML markup removed. \n",
    "# Use from urllib import request and then  request.urlopen('http://nltk.org/').read().decode('utf8') \n",
    "# to access the contents of the URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Natural Language Toolkit — NLTK 3.3 documentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NLTK 3.3 documentation\n",
      "\n",
      "next |\n",
      "          modules |\n",
      "          index\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Natural Language Toolkit¶\n",
      "NLTK is a leading platform for building Python programs to work with human language data.\n",
      "It provides easy-to-use interfaces to over 50 corpora and lexical\n",
      "resources such as WordNet,\n",
      "along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning,\n",
      "wrappers for industrial-strength NLP libraries,\n",
      "and an active discussion forum.\n",
      "Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation,\n",
      "NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike.\n",
      "NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project.\n",
      "NLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,”\n",
      "and “an amazing library to play with natural language.”\n",
      "Natural Language Processing with Python provides a practical\n",
      "introduction to programming for language processing.\n",
      "Written by the creators of NLTK, it guides the reader through the fundamentals\n",
      "of writing Python programs, working with corpora, categorizing text, analyzing linguistic structure,\n",
      "and more.\n",
      "The online version of the book has been been updated for Python 3 and NLTK 3.\n",
      "(The original Python 2 version is still available at http://nltk.org/book_1ed.)\n",
      "\n",
      "Some simple things you can do with NLTK¶\n",
      "Tokenize and tag some text:\n",
      ">>> import nltk\n",
      ">>> sentence = \"\"\"At eight o'clock on Thursday morning\n",
      "... Arthur didn't feel very good.\"\"\"\n",
      ">>> tokens = nltk.word_tokenize(sentence)\n",
      ">>> tokens\n",
      "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning',\n",
      "'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n",
      ">>> tagged = nltk.pos_tag(tokens)\n",
      ">>> tagged[0:6]\n",
      "[('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'JJ'), ('on', 'IN'),\n",
      "('Thursday', 'NNP'), ('morning', 'NN')]\n",
      "\n",
      "\n",
      "Identify named entities:\n",
      ">>> entities = nltk.chunk.ne_chunk(tagged)\n",
      ">>> entities\n",
      "Tree('S', [('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'JJ'),\n",
      "           ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'),\n",
      "       Tree('PERSON', [('Arthur', 'NNP')]),\n",
      "           ('did', 'VBD'), (\"n't\", 'RB'), ('feel', 'VB'),\n",
      "           ('very', 'RB'), ('good', 'JJ'), ('.', '.')])\n",
      "\n",
      "\n",
      "Display a parse tree:\n",
      ">>> from nltk.corpus import treebank\n",
      ">>> t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
      ">>> t.draw()\n",
      "\n",
      "\n",
      "\n",
      "NB. If you publish work that uses NLTK, please cite the NLTK book as\n",
      "follows:\n",
      "\n",
      "Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python.  O’Reilly Media Inc.\n",
      "\n",
      "\n",
      "Next Steps¶\n",
      "\n",
      "sign up for release announcements\n",
      "join in the discussion\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents¶\n",
      "\n",
      "\n",
      "NLTK News\n",
      "Installing NLTK\n",
      "Installing NLTK Data\n",
      "Contribute to NLTK\n",
      "FAQ\n",
      "Wiki\n",
      "API\n",
      "HOWTO\n",
      "\n",
      "\n",
      "\n",
      "Index\n",
      "Module Index\n",
      "Search Page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Table Of Contents\n",
      "\n",
      "NLTK News\n",
      "Installing NLTK\n",
      "Installing NLTK Data\n",
      "Contribute to NLTK\n",
      "FAQ\n",
      "Wiki\n",
      "API\n",
      "HOWTO\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "next |\n",
      "            modules |\n",
      "            index\n",
      "\n",
      "\n",
      "\n",
      "Show Source\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        © Copyright 2017, NLTK Project.\n",
      "      Last updated on May 06, 2018.\n",
      "      Created using Sphinx 1.7.4.\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Laure/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/Laure/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "def get_content(URL):\n",
    "    html = request.urlopen(URL).read().decode('utf8')\n",
    "    return BeautifulSoup(html).get_text()\n",
    "    \n",
    "print(get_content('http://nltk.org/'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise 9: Save some text into a file corpus.txt. Define a function load(f) that reads from the file named \n",
    "# in its sole argument, and returns a string containing the text of the file.\n",
    "\n",
    "# Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various kinds of punctuation in this text. Use one multi-line regular expression, with inline comments, using the verbose flag (?x).\n",
    "# Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following kinds of expression: monetary amounts; dates; names of people and organizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\\rtf1\\ansi\\ansicpg1252\\cocoartf1561\\cocoasubrtf600\n",
      "{\\fonttbl\\f0\\fswiss\\fcharset0 Helvetica;}\n",
      "{\\colortbl;\\red255\\green255\\blue255;\\red49\\green49\\blue49;\\red255\\green255\\blue255;\\red23\\green23\\blue23;\n",
      "}\n",
      "{\\*\\expandedcolortbl;;\\cssrgb\\c25098\\c25098\\c25098;\\cssrgb\\c100000\\c100000\\c100000;\\cssrgb\\c11765\\c11765\\c11765;\n",
      "}\n",
      "\\paperw11900\\paperh16840\\margl1440\\margr1440\\vieww10800\\viewh8400\\viewkind0\n",
      "\\deftab720\n",
      "\\pard\\pardeftab720\\sl320\\partightenfactor0\n",
      "\n",
      "\\f0\\b\\fs24 \\cf2 \\cb3 \\expnd0\\expndtw0\\kerning0\n",
      "\\outl0\\strokewidth0 \\strokec2 The Bank of England has raised the interest rate for only the second time in a decade.\\\n",
      "\\pard\\pardeftab720\\sl320\\partightenfactor0\n",
      "\n",
      "\\b0 \\cf2 The rate has risen by a quarter of a percentage point, from 0.5% to 0.75% - the highest level since March 2009.\\\n",
      "The move will increase the interest costs of more than three-and-a-half million residential mortgages that have variable or tracker rates.\\\n",
      "But it will be welcomed by savers, who will be hoping to see a rise in their interest rates over the coming months.\\\n",
      "Some business groups questioned the decision to raise the rate now ahead of the UK agreeing a Brexit deal with the European Union.\\\n",
      "Suren Thiru, head of economics at the British Chambers of Commerce, said: \"While a quarter-point rise may have a limited long-term financial impact on most businesses, it risks undermining confidence at a time of significant political and economic uncertainty.\"\\\n",
      "However, the Bank of England Governor Mark Carney told the BBC that the Monetary Policy Committee (MPC) would lower the rate if the situation merited such a move.\\\n",
      "\"There are a variety of scenarios that can happen with Brexit\\'85 but in many of those scenarios interest rates should be at least at these levels and so this decision is consistent with that,\" he said.\\\n",
      "\"In those scenarios where the interest rate should be lower, well then the MPC which meets eight times a year would, I'm confident, take the right decision to adjust interest rates at that time.\"\\\n",
      "\\pard\\pardeftab720\\sl420\\partightenfactor0\n",
      "\n",
      "\\fs36 \\cf4 \\strokec4 Why are they doing this now?\\\n",
      "\\pard\\pardeftab720\\sl320\\partightenfactor0\n",
      "\n",
      "\\fs24 \\cf2 \\strokec2 The Bank's MPC had been expected to raise interest rates in May, but held fire because the economy went through a weak patch at the start of the year - partly because of the harsh weather conditions, dubbed the Beast from the East.\\\n",
      "The Bank is now confident that the dip was temporary and that economic growth will recover from the 0.2% rate seen in the first quarter, to 0.4% in the second quarter and maintain that pace later in the year.\\\n",
      "The Bank is sticking to previous guidance that there will be further interest rate rises, but Mr Carney said these will be \"limited and gradual\".\\\n",
      "\"Rates can be expected to rise gradually. Policy needs to walk, not run, to stand still,\" he said.\\\n",
      "However, the Institute of Directors said the Bank had \"jumped the gun\" by raising the rate now.\\\n",
      "It said: \"The rise threatens to dampen consumer and business confidence at an already fragile time.\\\n",
      "\"Growth has remained subdued, and the recent partial rebound is the least that could be expected after the lack of progress in the year's first quarter.\"\\\n",
      "\\\n",
      "Etc}\n"
     ]
    }
   ],
   "source": [
    "file = open('corpus.txt')\n",
    "\n",
    "def load(f):\n",
    "    return f.read()\n",
    "\n",
    "text = load(file)\n",
    "\n",
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various kinds of punctuation in this text. \n",
    "# Use one multi-line regular expression, with inline comments, using the verbose flag (?x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{', '{', ';}', '{', ';', ';', ';', ';', ';', '}', '{', ';;', ';', ';', ';', '}', '.', ',', '.', '.', '-', '.', '-', '-', '-', '.', ',', '.', '.', ',', ',', ':', '\"', '-', '-', ',', '.\"', ',', '(', ')', '.', '\"', \"'\", ',\"', '.', '\"', ',', ',', \"'\", ',', '.\"', '?', \"'\", ',', '-', ',', '.', '.', ',', '.', '.', ',', '\"', '\".', '\"', '.', ',', ',', ',\"', '.', ',', '\"', '\"', '.', ':', '\"', '.', '\"', ',', \"'\", '.\"', '}']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'''(?x)\n",
    "    [\\.,;\"'?\\(\\):\\-_`\\[\\]\\{\\}]+ # one or more punctuation symbols, brackets etc.\n",
    "'''\n",
    "print(nltk.regexp_tokenize(text, pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following kinds of expression: monetary amounts; \n",
    "# dates; names of people and organizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Monetary', '£10', '01-08-2018', '1 August 2018', 'Laure', 'United Kingdom']\n"
     ]
    }
   ],
   "source": [
    "pattern2 = r'''(?x)\n",
    "    \\£\\s?(?:\\d+\\.)?\\d+                  # Monetary ex. £5\n",
    "    | \\d{2}\\-\\d{2}\\-\\d{4}               # Date ex. 01-08-2018\n",
    "    | \\d{1,2}\\s[A-Z][a-z]{2,8}\\s\\d{4}   # Date ex. 1 August 2018\n",
    "    | ([A-Z][a-z]*\\s[A-Z][a-z]*)        # Names\n",
    "'''\n",
    "\n",
    "# Test \n",
    "test = 'Monetary example is £10 or $2.20, date example is or 01-08-2018 or 1 August 2018 and name example is Laure or United Kingdom'\n",
    "print (nltk.regexp_tokenize(test, pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Bank', 'European Union', 'Suren Thiru', 'British Chambers', 'England Governor', 'Mark Carney', 'Monetary Policy', 'The Bank', 'The Bank', 'The Bank', 'Mr Carney']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.regexp_tokenize(text, pattern2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise 10: Rewrite the following loop as a list comprehension:\n",
    "\n",
    " \t\n",
    "# >>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "# >>> result = []\n",
    "# >>> for word in sent:\n",
    "# ...     word_len = (word, len(word))\n",
    "# ...     result.append(word_len)\n",
    "# >>> result\n",
    "# [('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', '3'), ('dog', '3'), ('gave', '4'), ('John', '4'), ('the', '3'), ('newspaper', '9')]\n"
     ]
    }
   ],
   "source": [
    "print(list((w, str(len(w))) for w in ['The', 'dog', 'gave', 'John', 'the', 'newspaper']))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
